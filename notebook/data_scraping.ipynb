{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa8d35d-f11a-40a9-85ae-1748263aecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical Data Loaded. Shape: (4253, 5)\n",
      "          company   industry  total_laid_off       date        country\n",
      "0         Sapiens    Finance           700.0 2025-12-28         Israel\n",
      "1       Yellow.ai    Support           100.0 2025-12-23  United States\n",
      "2  The Trade Desk  Marketing             NaN 2025-12-17  United States\n",
      "3          Amazon     Retail            84.0 2025-12-15  United States\n",
      "4     PowerSchool  Education             NaN 2025-12-12  United States\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_hist = pd.read_csv('layoffs_historical.csv')\n",
    "\n",
    "df_hist = df_hist[['company', 'industry', 'total_laid_off', 'date', 'country']]\n",
    "\n",
    "# Convert Date to standard datetime objects\n",
    "df_hist['date'] = pd.to_datetime(df_hist['date'])\n",
    "\n",
    "print(\"Historical Data Loaded. Shape:\", df_hist.shape)\n",
    "print(df_hist.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894b5d8f-7fd0-4eae-9483-86f31fe96bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd05abee-c189-4e01-bd01-5f49226d0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening https://peerlist.io/layoffs-tracker...\n",
      "âœ… Success! Scraped 4 rows for 2026.\n"
     ]
    }
   ],
   "source": [
    "def get_2026_data_resilient():\n",
    "    # 1. Setup Driver\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\") # Keep off to watch for Captchas\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    url = \"https://peerlist.io/layoffs-tracker\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Opening {url}...\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # 2. WAIT for the table to appear (up to 20 seconds)\n",
    "        wait = WebDriverWait(driver, 20)\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "        \n",
    "        # 3. Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if table is None:\n",
    "            print(\"âŒ Error: Table still not found after waiting.\")\n",
    "            return None\n",
    "\n",
    "        # 4. Extract Data\n",
    "        rows = table.find_all('tr')\n",
    "        data = []\n",
    "        for row in rows[1:]: # Skip header\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) >= 4:\n",
    "                data.append({\n",
    "                    'Company': cols[0].text.strip(),\n",
    "                    'Laid_Off_Count': cols[1].text.strip().split(' ')[0],\n",
    "                    'Date': cols[2].text.strip(),\n",
    "                    'Industry': cols[3].text.strip(),\n",
    "                    'Country': cols[4].text.strip() if len(cols) > 4 else \"N/A\"\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"peerlist_2026.csv\", index=False)\n",
    "        print(f\"âœ… Success! Scraped {len(df)} rows for 2026.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ›‘ Error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Run it\n",
    "df_2026 = get_2026_data_resilient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec6788-fb92-418c-8694-ec6c96e6a9c8",
   "metadata": {},
   "source": [
    "Merging 2 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae53a8a-9ebd-44f1-a305-6ace721408e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_clean = df_hist.rename(columns={\n",
    "    'total_laid_off': 'Laid_Off_Count',\n",
    "    'country': 'Country',\n",
    "    'date': 'Date',\n",
    "    'company': 'Company',\n",
    "    'industry' : 'Industry'\n",
    "})\n",
    "\n",
    "df_2026_clean = df_2026.rename(columns={\n",
    "    'Layoff_Count': 'Laid_Off_Count',\n",
    "    'Location': 'Country'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bfb4af1-7987-4e5b-8eb6-0dfb092049c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2026_clean['Laid_Off_Count'] = df_2026_clean['Laid_Off_Count'].astype(str).str.split(' ').str[0]\n",
    "\n",
    "# 2. Convert counts to numbers (float handles 'NaN' better than int)\n",
    "df_hist_clean['Laid_Off_Count'] = pd.to_numeric(df_hist_clean['Laid_Off_Count'], errors='coerce')\n",
    "df_2026_clean['Laid_Off_Count'] = pd.to_numeric(df_2026_clean['Laid_Off_Count'], errors='coerce')\n",
    "\n",
    "# 3. Standardize Dates\n",
    "df_hist_clean['Date'] = pd.to_datetime(df_hist_clean['Date'])\n",
    "df_2026_clean['Date'] = pd.to_datetime(df_2026_clean['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91640c4f-06f3-43fc-b368-b1a2363cb65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Complete! Total records in master dataset: 4251\n"
     ]
    }
   ],
   "source": [
    "df_master = pd.concat([df_hist_clean, df_2026_clean], ignore_index=True)\n",
    "\n",
    "# Drop any potential duplicates (if a 2025 layoff was in both sets)\n",
    "df_master = df_master.drop_duplicates(subset=['Company', 'Date', 'Laid_Off_Count'])\n",
    "\n",
    "# Sort by date to see the timeline clearly\n",
    "df_master = df_master.sort_values(by='Date', ascending=False)\n",
    "\n",
    "# Save your final \"Single Source of Truth\"\n",
    "df_master.to_csv(\"Layoffs_2022-2026.csv\", index=False)\n",
    "\n",
    "print(f\"Merge Complete! Total records in master dataset: {len(df_master)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f346e2-0c2d-4303-91ff-eda322114816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
